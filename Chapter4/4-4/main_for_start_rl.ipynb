{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gymnasium\n",
    "# pip install gym\n",
    "# pip install swig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用強化學習方法來尋找 0050.TW 這檔標的的買賣時間點。\\\n",
    "初始資金設置為 20000 元。\\\n",
    "訓練集數據範圍為 2020/1/1 至 2020/12/31，測試集數據範圍為 2021/1/1 至 2021/12/31。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入所需套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yfinance as yf\n",
    "from stable_baselines3 import A2C, DQN, PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 gym 建置環境的架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化環境，設定初始參數。\n",
    "        這個方法主要負責初始化環境中的 State 或是 Action 等。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置環境，將所有狀態重置為初始值。這個方法通常在每一個新的 Episode 開始時被調用。\"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        根據執行的 Action 來更新環境的 State，並計算 Reward。\n",
    "        同時也會檢查當前的 Episode 是否結束。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"顯示資訊。\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init 方法範例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "print(observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Box(low=0, high=255, shape=(64, 64, 3))\n",
    "print(observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(n=3)\n",
    "print(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化環境，設定初始參數。\n",
    "        這個方法主要負責初始化環境中的 State 或是 Action 等。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置環境，將所有狀態重置為初始值。這個方法通常在每一個新的 Episode 開始時被調用。\"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        根據執行的 Action 來更新環境的 State，並計算 Reward。\n",
    "        同時也會檢查當前的 Episode 是否結束。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"顯示資訊。\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reset 方法範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化環境，設定初始參數。\n",
    "        這個方法主要負責初始化環境中的 State 或是 Action 等。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置環境，將所有狀態重置為初始值。這個方法通常在每一個新的 Episode 開始時被調用。\"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        根據執行的 Action 來更新環境的 State，並計算 Reward。\n",
    "        同時也會檢查當前的 Episode 是否結束。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"顯示資訊。\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 方法範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化環境，設定初始參數。\n",
    "        這個方法主要負責初始化環境中的 State 或是 Action 等。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置環境，將所有狀態重置為初始值。這個方法通常在每一個新的 Episode 開始時被調用。\"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        根據執行的 Action 來更新環境的 State，並計算 Reward。\n",
    "        同時也會檢查當前的 Episode 是否結束。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"顯示資訊。\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### render 方法範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化環境，設定初始參數。\n",
    "        這個方法主要負責初始化環境中的 State 或是 Action 等。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置環境，將所有狀態重置為初始值。這個方法通常在每一個新的 episode 開始時被調用。\"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        根據執行的 Action 來更新環境的 State，並計算 Reward。\n",
    "        同時也會檢查當前的 episode 是否結束。\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"顯示資訊。\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先準備訓練資料和測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下載 0050.TW 的歷史資料，時間範圍為 2020-01-01 至 2020-12-31 當作訓練資料\n",
    "train_df = (\n",
    "    pd.DataFrame(yf.download(\"0050.TW\", start=\"2020-01-01\", end=\"2020-12-31\"))\n",
    "    .droplevel(\"Ticker\", axis=1)\n",
    "    .reset_index()\n",
    "    .ffill()\n",
    ")\n",
    "train_df.index = train_df[\"Date\"]\n",
    "train_df.columns.name = None\n",
    "train_df = train_df.drop(columns=[\"Date\"])\n",
    "\n",
    "\n",
    "# 下載 0050.TW 的歷史資料，時間範圍為 2021-01-01 至 2021-12-31 當作測試資料\n",
    "test_df = (\n",
    "    pd.DataFrame(yf.download(\"0050.TW\", start=\"2021-01-01\", end=\"2021-12-31\"))\n",
    "    .droplevel(\"Ticker\", axis=1)\n",
    "    .reset_index()\n",
    "    .ffill()\n",
    ")\n",
    "test_df.index = test_df[\"Date\"]\n",
    "test_df.columns.name = None\n",
    "test_df = test_df.drop(columns=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境建置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        初始化股票交易環境。\n",
    "        設定交易環境中的參數和狀態，包括初始資金、手續費、動作空間和觀察空間。\n",
    "        \"\"\"\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.df = df  # 股票的歷史數據\n",
    "        self.stock_dim = 1  # 股票數量，這裡設定為1（如0050.TW）\n",
    "        self.initial_amount = 20000  # 初始資金設定為 20000\n",
    "        self.buy_cost_pct = 0.001425  # 買入股票時的手續費\n",
    "        self.sell_cost_pct = 0.001425 + 0.003  # 賣出股票時的手續費和交易稅\n",
    "        self.reward_scaling = 1e-2  # Reward 的縮放(正規化)比例\n",
    "\n",
    "        # 定義 Action 一維空間，表示賣出和買入的動作\n",
    "        # 數值 0 表示執行賣出動作\n",
    "        # 數值 1 表示保持不變\n",
    "        # 數值 2 表示執行買入動作\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "\n",
    "        # 定義 State 七維空間，State 包含現金、持有的股票數量、股票價格數據(開高低收量)\n",
    "        # State 可能數值範圍從0到無限大\n",
    "        # State[0]:現金； State[1]:持有的股票數量； State[2]:開盤價； State[3]:最高價；\n",
    "        # State[4]:最低價； State[5]:收盤價； State[6]:交易量\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=np.inf, shape=(2 + len(self.df.columns),), dtype=np.float32\n",
    "        )\n",
    "        # 重置 Environment State 為初始 State\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置環境狀態為初始狀態，通常在每個 Episode 的開始時調用。\n",
    "        初始化現金、持有股票數量、第一天的股票價格等。\n",
    "        \"\"\"\n",
    "        # 設置隨機種子\n",
    "        random.seed(7777)\n",
    "        np.random.seed(7777)\n",
    "        torch.manual_seed(7777)\n",
    "\n",
    "        self.day = 0  # 初始化執行天數為第一天\n",
    "        # 取出第一天歷史資料當作初始股票價格數據\n",
    "        self.data = self.df.iloc[self.day]\n",
    "\n",
    "        # 初始化現金 State[0] 為初始金額 self.initial_amount\n",
    "        # 初始化持有的股票數量 State[1] = 0\n",
    "        # 初始化股票價格數據(開高低收量) State[2]~State[6] 是第一天歷史資料\n",
    "        self.state = np.array(\n",
    "            [self.initial_amount, 0] + self.data.tolist(), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # self.terminal 的值用來判斷交易是否應該結束，如果是 True 就是一個 Episode 結束\n",
    "        # 重置終止標誌為 False，表示新的一輪交易開始\n",
    "        self.terminal = False\n",
    "\n",
    "        self.reward = 0  # 初始化累積收益值為0\n",
    "        self.asset_memory = [self.initial_amount]  # 初始化資產記錄為初始資金\n",
    "        self.actions_memory = []  # 初始化 Action 記錄\n",
    "        self.date_memory = [self.data.name]  # 初始化日期記錄為第一天日期\n",
    "        self.trade_memory = []  # 初始化交易記錄\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        根據 Action 來更新環境狀態，計算回報，並判斷 Episode 是否結束。\n",
    "        \"\"\"\n",
    "        # 檢查是否到達資料的最後一天，若到達最後一天，則設置 terminal 為 True\n",
    "        self.terminal = self.day >= len(self.df.index) - 1\n",
    "        if self.terminal:\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        # 計算當前總資產(現金+股票的價值)\n",
    "        begin_total_asset = self.state[0] + self.state[1] * self.state[2]\n",
    "\n",
    "        if action == 0:  # 執行賣出操作\n",
    "            # 賣出的股票數量是目前持有的股票數量\n",
    "            sell_num_shares = self.state[1]\n",
    "            # 計算賣出股票後獲得的現金(數量*開盤價)，並扣除賣出手續費\n",
    "            sell_amount = sell_num_shares * self.state[2] * (1 - self.sell_cost_pct)\n",
    "            self.state[0] += sell_amount  # 更新現金餘額\n",
    "            self.state[1] -= sell_num_shares  # 更新持有的股票數量\n",
    "            # 記錄賣出交易的日期、類型、數量和價格\n",
    "            self.trade_memory.append(\n",
    "                (self.data.name, \"sell\", sell_num_shares, self.state[2])\n",
    "            )\n",
    "\n",
    "        if action == 2:  # 執行買入操作\n",
    "            # 計算能夠買入的股票數量，不能超過可用的現金數量(現金除以當下價格)\n",
    "            buy_num_shares = self.state[0] // (self.state[2] * (1 + self.buy_cost_pct))\n",
    "            # 計算買入股票需要的現金(數量*開盤價)，並加上買入手續費\n",
    "            buy_amount = buy_num_shares * self.state[2] * (1 + self.buy_cost_pct)\n",
    "            self.state[0] -= buy_amount  # 更新現金餘額\n",
    "            self.state[1] += buy_num_shares  # 更新持有的股票數量\n",
    "            # 記錄買入交易的日期、類型、數量和價格\n",
    "            self.trade_memory.append(\n",
    "                (self.data.name, \"buy\", buy_num_shares, self.state[2])\n",
    "            )\n",
    "\n",
    "        self.day += 1  # 更新天數到下一天\n",
    "        self.data = self.df.iloc[self.day]  # 取得下一天的數據\n",
    "        # 更新 State，包括目前現金、目前持有的股票數量和下一天股票價格數據\n",
    "        self.state = np.array(\n",
    "            [self.state[0], self.state[1]] + self.data.tolist(), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # 計算新的總資產\n",
    "        end_total_asset = self.state[0] + self.state[1] * self.state[2]\n",
    "\n",
    "        # 計算 Reward 為總資產的變化，並將 Reward 進行正規化動作\n",
    "        self.reward = end_total_asset - begin_total_asset\n",
    "        self.reward = self.reward * self.reward_scaling\n",
    "\n",
    "        # 根據不同的動作給予額外的小額獎勵\n",
    "        if action == 0:  # 賣出獲得額外獎勵\n",
    "            self.reward += 0.05\n",
    "        elif action == 1:  # 保持獲得小額獎勵\n",
    "            self.reward += 0.005\n",
    "\n",
    "        # 記錄資產、日期和 Action 的變化\n",
    "        self.asset_memory.append(end_total_asset)\n",
    "        self.date_memory.append(self.data.name)\n",
    "        self.actions_memory.append(action)\n",
    "\n",
    "        # 回傳當下 State、Reward、是否終止和其他額外訊息\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"顯示交易環境的當前狀態。\"\"\"\n",
    "        print(f\"目前日期: {self.data.name}\")\n",
    "        print(f\"目前執行到第幾天: {self.day}\")\n",
    "        print(f\"目前Reward: {self.reward}\")\n",
    "        print(f\"目前現金金額: {self.state[0]}\")\n",
    "        print(f\"目前持有股票數量: {self.state[1]}\")\n",
    "        print(f\"目前股票開盤價: {self.state[2]}\")\n",
    "        print(f\"目前總資產: {self.state[0] + self.state[1] * self.state[2]}\")\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        \"\"\"儲存每一天的總資產記錄，並回傳一個包含日期和資產的 DataFrame。\"\"\"\n",
    "        return pd.DataFrame({\"date\": self.date_memory, \"asset\": self.asset_memory})\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        \"\"\"儲存每一天的 Action 記錄，並回傳一個包含日期和動作的 DataFrame。\"\"\"\n",
    "        return pd.DataFrame(\n",
    "            {\"date\": self.date_memory[:-1], \"actions\": self.actions_memory}\n",
    "        )\n",
    "\n",
    "    def save_trade_memory(self):\n",
    "        \"\"\"儲存發生交易的記錄，並回傳一個包含日期、交易類型、交易股票數量和交易價格的 DataFrame。\"\"\"\n",
    "        return pd.DataFrame(\n",
    "            self.trade_memory, columns=[\"date\", \"type\", \"shares\", \"price\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Based Method\n",
    "- Proximal Policy Optimization(PPO)：適用於連續和離散動作空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用訓練數據集 train_df 創建交易環境\n",
    "env = StockTradingEnv(train_df)\n",
    "\n",
    "# 使用 PPO 算法創建模型\n",
    "# 使用多層感知機（Multi-Layer Perceptron, MLP）作為策略和價值網絡的結構\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 訓練模型，總訓練步數為 30000 步\n",
    "model.learn(total_timesteps=30000)\n",
    "\n",
    "# 將訓練好的模型儲存為文件 PPO_0050.zip\n",
    "model.save(\"PPO_0050\")\n",
    "\n",
    "# 載入儲存的模型\n",
    "model = PPO.load(\"PPO_0050\")\n",
    "\n",
    "# 評估模型在訓練集上的表現\n",
    "obs = env.reset()\n",
    "for i in range(len(train_df)):\n",
    "    action, _states = model.predict(obs)  # 使用模型預測動作\n",
    "    obs, rewards, done, info = env.step(action)  # 執行動作\n",
    "    env.render()  # 顯示環境狀態\n",
    "\n",
    "# 儲存交易結果\n",
    "asset_memory = env.save_asset_memory()  # 儲存資產變化記錄\n",
    "asset_memory.to_csv(\"PPO_assets.csv\")\n",
    "\n",
    "action_memory = env.save_action_memory()  # 儲存 Action 記錄\n",
    "action_memory.to_csv(\"PPO_actions.csv\")\n",
    "\n",
    "trade_memory = env.save_trade_memory()  # 儲存交易記錄\n",
    "trade_memory.to_csv(\"PPO_trades.csv\")\n",
    "\n",
    "# 畫出資產變化圖\n",
    "plt.plot(asset_memory[\"date\"], asset_memory[\"asset\"])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Asset Value\")\n",
    "plt.title(\"Cumulative Asset Value over Time (PPO)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 顯示交易記錄\n",
    "# 顯示買入進場的時間點\n",
    "buy_df = trade_memory[(trade_memory[\"type\"] == \"buy\")]\n",
    "buy_df = buy_df[(trade_memory[\"shares\"] != 0)]\n",
    "print(\"買入進場的時間點:\")\n",
    "print(buy_df)\n",
    "\n",
    "# 顯示賣出出場的時間點\n",
    "sell_df = trade_memory[(trade_memory[\"type\"] == \"sell\")]\n",
    "sell_df = sell_df[(trade_memory[\"shares\"] != 0)]\n",
    "print(\"賣出出場的時間點:\")\n",
    "print(sell_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic Method\n",
    "- Advantage Actor-Critic(A2C)：適用於連續和離散動作空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用訓練數據集 train_df 創建交易環境\n",
    "env = StockTradingEnv(train_df)\n",
    "\n",
    "# 使用 A2C 算法創建模型\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 訓練模型，總訓練步數為 30000 步\n",
    "model.learn(total_timesteps=30000)\n",
    "\n",
    "# 將訓練好的模型儲存為文件 A2C_0050.zip\n",
    "model.save(\"A2C_0050\")\n",
    "\n",
    "# 載入儲存的模型\n",
    "model = A2C.load(\"A2C_0050\")\n",
    "\n",
    "# 評估模型在訓練集上的表現\n",
    "obs = env.reset()\n",
    "for i in range(len(train_df)):\n",
    "    action, _states = model.predict(obs)  # 使用模型預測動作\n",
    "    obs, rewards, done, info = env.step(action)  # 執行動作\n",
    "    env.render()  # 顯示環境狀態\n",
    "\n",
    "# 儲存交易結果\n",
    "asset_memory = env.save_asset_memory()  # 儲存資產變化記錄\n",
    "asset_memory.to_csv(\"A2C_0050_assets.csv\")\n",
    "\n",
    "action_memory = env.save_action_memory()  # 儲存 Action 記錄\n",
    "action_memory.to_csv(\"A2C_0050_actions.csv\")\n",
    "\n",
    "trade_memory = env.save_trade_memory()  # 儲存交易記錄\n",
    "trade_memory.to_csv(\"A2C_0050_trades.csv\")\n",
    "\n",
    "# 畫出資產變化圖\n",
    "plt.plot(asset_memory[\"date\"], asset_memory[\"asset\"])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Asset Value\")\n",
    "plt.title(\"Cumulative Asset Value over Time (A2C)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 顯示交易記錄\n",
    "# 顯示買入進場的時間點\n",
    "buy_df = trade_memory[(trade_memory[\"type\"] == \"buy\")]\n",
    "buy_df = buy_df[(trade_memory[\"shares\"] != 0)]\n",
    "print(\"買入進場的時間點:\")\n",
    "print(buy_df)\n",
    "\n",
    "# 顯示賣出出場的時間點\n",
    "sell_df = trade_memory[(trade_memory[\"type\"] == \"sell\")]\n",
    "sell_df = sell_df[(trade_memory[\"shares\"] != 0)]\n",
    "print(\"賣出出場的時間點:\")\n",
    "print(sell_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valued Based Method\n",
    "- Deep Q-Network(DQN)：適用於離散動作空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用訓練數據集 train_df 創建交易環境\n",
    "env = StockTradingEnv(train_df)\n",
    "\n",
    "# 使用 DQN 算法創建模型\n",
    "model = DQN(policy=\"MlpPolicy\", env=env, verbose=1)\n",
    "\n",
    "# 訓練模型，總訓練步數為 1000 步\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# 將訓練好的模型儲存為文件 DQN_0050.zip\n",
    "model.save(\"DQN_0050\")\n",
    "\n",
    "# 載入儲存的模型\n",
    "model = DQN.load(\"DQN_0050\")\n",
    "\n",
    "# 評估模型在訓練集上的表現\n",
    "obs = env.reset()\n",
    "for i in range(len(train_df)):\n",
    "    action, _states = model.predict(obs)  # 使用模型預測動作\n",
    "    obs, rewards, done, info = env.step(action)  # 執行動作\n",
    "    env.render()  # 顯示環境狀態\n",
    "\n",
    "# 儲存交易結果\n",
    "asset_memory = env.save_asset_memory()  # 儲存資產變化記錄\n",
    "asset_memory.to_csv(\"DQN_0050_assets.csv\")\n",
    "\n",
    "action_memory = env.save_action_memory()  # 儲存 Action 記錄\n",
    "action_memory.to_csv(\"DQN_0050_actions.csv\")\n",
    "\n",
    "trade_memory = env.save_trade_memory()  # 儲存交易記錄\n",
    "trade_memory.to_csv(\"DQN_0050_trades.csv\")\n",
    "\n",
    "# 畫出資產變化圖\n",
    "plt.plot(asset_memory[\"date\"], asset_memory[\"asset\"])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Asset Value\")\n",
    "plt.title(\"Cumulative Asset Value over Time (DQN)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 顯示交易記錄\n",
    "# 顯示買入進場的時間點\n",
    "buy_df = trade_memory[(trade_memory[\"type\"] == \"buy\")]\n",
    "buy_df = buy_df[(trade_memory[\"shares\"] != 0)]\n",
    "print(\"買入進場的時間點:\")\n",
    "print(buy_df)\n",
    "\n",
    "# 顯示賣出出場的時間點\n",
    "sell_df = trade_memory[(trade_memory[\"type\"] == \"sell\")]\n",
    "sell_df = sell_df[(trade_memory[\"shares\"] != 0)]\n",
    "print(\"賣出出場的時間點:\")\n",
    "print(sell_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bookenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
