{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定義 Dataset 模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        # 這個方法是用來初始化資料集。\n",
    "        # 參數data是輸入資料，參數seq_length是設定每個樣本特徵的序列長度。\n",
    "        # 舉例來說，如果想使用前3天的開高低收量來預測隔天的收盤價，\n",
    "        # 就可以將 seq_length 設為 3。\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # 這個方法是回傳資料集的樣本數。\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 這個方法是用來根據索引回傳資料。把要回傳的資料放在 return 後面。\n",
    "        # self.data 第一個位置索引對應的是時間軸\n",
    "        # self.data 第二個位置索引對應的是欄位（0:開,1:高,2:低,3:收,4:量）\n",
    "        x = self.data[idx : (idx + self.seq_length), [0, 1, 2, 3, 4]]\n",
    "        y = self.data[idx + self.seq_length, 3]\n",
    "        return (\n",
    "            torch.tensor(x, dtype=torch.float32),\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "# 創建資料集實例\n",
    "train_dataset = StockDataset(data=train_data, seq_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 創建數據加載器\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# 可以透過 for 迴圈來迭代顯示數據加載器內的資料\n",
    "for batch_data in train_loader:\n",
    "    x, y = batch_data\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方誤差（Mean Squared Error；MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2875000834465027\n",
      "Mean Squared Error: 0.2875000834465027\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定義均方誤差損失函式\n",
    "mse_loss = nn.MSELoss()\n",
    "# 假設有一些預測值 predictions 和真實值 targets\n",
    "predictions = torch.tensor([2.5, 0.0, 2.1, 7.8], dtype=torch.float32)\n",
    "targets = torch.tensor([3.0, -0.5, 2.0, 7.0], dtype=torch.float32)\n",
    "# 計算均方誤差\n",
    "loss = mse_loss(predictions, targets)\n",
    "print(f\"Mean Squared Error: {loss.item()}\")\n",
    "# Mean Squared Error: 0.2875000834465027\n",
    "print(f\"Mean Squared Error: {loss}\")\n",
    "# Mean Squared Error: 0.2875000834465027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二元交叉熵損失（Binary Cross Entropy Loss）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross Entropy Loss: 0.3779643774032593\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定義二元交叉熵損失函式\n",
    "bce_loss = nn.BCELoss()\n",
    "# 假設有一些預測機率 predictions（值域是 0 到 1） 和真實值 targets（0 或 1）\n",
    "predictions = torch.tensor([0.5, 0.3, 0.9, 0.7], dtype=torch.float32)\n",
    "targets = torch.tensor([1, 0, 1, 1], dtype=torch.float32)\n",
    "# 計算二元交叉熵損失\n",
    "loss = bce_loss(predictions, targets)\n",
    "print(f\"Binary Cross Entropy Loss: {loss.item()}\")\n",
    "# Binary Cross-Entropy Loss: 0.3779643774032593"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵損失（Cross Entropy Loss）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 1.3144149780273438\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定義交叉熵損失函式\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "# 假設有一些預測機率 predictions（範圍是 0 到 1） 和真實值 targets（四類）\n",
    "predictions = torch.tensor(\n",
    "    [[0.25, 0.25, 0.25, 0.25], [0.1, 0.2, 0.3, 0.4]], dtype=torch.float32\n",
    ")\n",
    "targets = torch.tensor([1, 3], dtype=torch.long)\n",
    "# 計算交叉熵損失\n",
    "loss = cross_entropy_loss(predictions, targets)\n",
    "print(f\"Cross Entropy Loss: {loss.item()}\")\n",
    "# Cross-Entropy Loss: 1.3144149780273438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假設有 5 個類別，類別標籤為 0, 1, 2, 3, 4\n",
    "labels = torch.tensor([0, 1, 2, 3, 4])\n",
    "# 將類別標籤轉換為 One-Hot Encoding\n",
    "one_hot_encoded = torch.nn.functional.one_hot(labels, num_classes=5)\n",
    "print(one_hot_encoded)\n",
    "type(one_hot_encoded)  # <class 'torch.Tensor'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假設有 5 個類別，類別標籤為 0, 1, 2, 3, 4\n",
    "labels = np.array([0, 1, 2, 3, 4])\n",
    "# 將類別標籤轉換為 One-Hot Encoding\n",
    "one_hot_encoded = np.eye(5)[labels]\n",
    "print(one_hot_encoded)\n",
    "type(one_hot_encoded)  # <class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隨機梯度下降（Stochastic Gradient Descent；SGD）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定義一個簡單的線性回歸模型\n",
    "# 假設輸入數據有兩個特徵，輸出是一個目標值\n",
    "model = torch.nn.Linear(2, 1)\n",
    "# 定義損失函數\n",
    "# 這裡使用均方誤差（MSE）作為損失函數，適合回歸問題\n",
    "criterion = torch.nn.MSELoss()\n",
    "# 定義優化器\n",
    "# 使用隨機梯度下降（SGD）優化算法，學習率設為 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# 準備數據\n",
    "# 輸入數據包含兩個樣本，每個樣本有兩個特徵\n",
    "data = torch.tensor([[1000, 3], [1500, 4]], dtype=torch.float32)\n",
    "# 目標值（真實標籤），每個樣本對應一個目標值\n",
    "targets = torch.tensor([[320000], [430000]], dtype=torch.float32)\n",
    "# 清除上一輪計算中累積的梯度訊息\n",
    "# 這一步很重要，因為 PyTorch 中的梯度是累積的，如果不清零會導致梯度被疊加\n",
    "optimizer.zero_grad()\n",
    "# 模型預測\n",
    "# 將輸入數據傳入模型，得到模型的預測輸出\n",
    "outputs = model(data)\n",
    "# 計算損失值\n",
    "# 使用定義的損失函數比較模型輸出（outputs）和目標值（targets）\n",
    "# 得到當前模型的損失值，用於衡量預測值與真實值的差異\n",
    "loss = criterion(outputs, targets)\n",
    "# 根據損失值對模型的參數計算梯度，用於指導參數的更新方向\n",
    "loss.backward()\n",
    "# 更新模型參數\n",
    "# 根據計算出的梯度，使用優化器更新模型的參數，從而減小損失值\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自適應矩估計（Adaptive Moment Estimation；Adam）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 143459155968.0\n",
      "Epoch 11/100, Loss: 143362392064.0\n",
      "Epoch 21/100, Loss: 143265660928.0\n",
      "Epoch 31/100, Loss: 143168995328.0\n",
      "Epoch 41/100, Loss: 143072346112.0\n",
      "Epoch 51/100, Loss: 142975762432.0\n",
      "Epoch 61/100, Loss: 142879227904.0\n",
      "Epoch 71/100, Loss: 142782742528.0\n",
      "Epoch 81/100, Loss: 142686306304.0\n",
      "Epoch 91/100, Loss: 142589902848.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定義一個簡單的線性回歸模型\n",
    "# 假設輸入數據包含兩個特徵，模型輸出是一個目標值\n",
    "# 模型表達式為 y = Wx + b，其中 W 是權重，b 是偏置\n",
    "model = torch.nn.Linear(2, 1)\n",
    "# 定義損失函數\n",
    "# 使用均方誤差（MSE）作為損失函數，用於衡量模型預測值與真實目標值的差異\n",
    "criterion = torch.nn.MSELoss()\n",
    "# 定義優化器\n",
    "# 使用 Adam 優化器，它是一種基於自適應學習率的優化算法，比標準的 SGD 收斂更快且穩定\n",
    "# 設定學習率 lr=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# 準備數據\n",
    "# 定義輸入數據和目標值\n",
    "# data 是輸入特徵的數據張量，每行是一個樣本，每列是一個特徵\n",
    "data = torch.tensor([[1000, 3], [1500, 4]], dtype=torch.float32)\n",
    "# targets 是輸入數據對應的真實目標值\n",
    "# 每個樣本對應一個目標值，表示模型需要學習的結果\n",
    "targets = torch.tensor([[320000], [430000]], dtype=torch.float32)\n",
    "# 訓練模型\n",
    "# 使用一個簡單的循環（epoch）來訓練模型，共進行 100 次迭代\n",
    "for epoch in range(100):\n",
    "    # 清除上一輪計算中累積的梯度\n",
    "    # PyTorch 中的梯度是累積的，因此在每次反向傳播前需要先清零\n",
    "    optimizer.zero_grad()\n",
    "    # 模型預測\n",
    "    # 將輸入數據傳入模型，得到模型的預測輸出\n",
    "    outputs = model(data)\n",
    "    # 計算損失值\n",
    "    # 使用 MSE 損失函數比較模型預測值（outputs）與目標值（targets）\n",
    "    # 損失值用於衡量當前模型的預測誤差大小\n",
    "    loss = criterion(outputs, targets)\n",
    "    # 計算梯度\n",
    "    # 根據損失值對模型參數計算梯度，用於指導模型參數更新的方向\n",
    "    loss.backward()\n",
    "    # 更新模型參數\n",
    "    # 使用 Adam 優化器根據梯度和學習率，更新模型的權重和偏置\n",
    "    optimizer.step()\n",
    "    # 用於觀察模型的訓練進度\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線性層（Linear Layer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定義模型\n",
    "# 使用 PyTorch 的 nn.Linear 定義一個全連接線性層（Linear Layer）\n",
    "# - `in_features=20` 表示輸入特徵的數量為 20\n",
    "# - `out_features=30` 表示輸出的特徵數量為 30\n",
    "# 這個層將實現一個線性變換：y = xW^T + b，其中 W 是權重，b 是偏置\n",
    "model = nn.Linear(in_features=20, out_features=30)\n",
    "# 創建輸入張量\n",
    "# 使用 torch.randn 隨機生成一個 2 維輸入張量，大小為 128 x 20\n",
    "# - 第一個維度（128）表示有 128 個樣本\n",
    "# - 第二個維度（20）表示每個樣本有 20 個特徵\n",
    "# 可以將這個張量想像成一組批量的輸入數據，每行是一個樣本，每列是一個特徵\n",
    "input = torch.randn(128, 20)\n",
    "type(input)  # <class 'torch.Tensor'>\n",
    "input.size()  # torch.Size([128, 20])\n",
    "# 將輸入張量傳入模型\n",
    "# 將 input 作為輸入傳遞給線性層，得到輸出張量 output\n",
    "# 線性層會對輸入張量進行線性變換，輸出張量的形狀會根據 out_features 調整\n",
    "output = model(input)\n",
    "type(output)  # <class 'torch.Tensor'>\n",
    "output.size()  # torch.Size([128, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷積層（Convolutional Layer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定義一個一維卷積層\n",
    "model = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=1)\n",
    "# 創建輸入張量\n",
    "# 使用 torch.randn 隨機生成一個一維輸入張量，大小為 1x10\n",
    "# - 第一個維度（1）表示輸入的通道數\n",
    "# - 第二個維度（10）表示輸入的特徵數\n",
    "input = torch.randn(1, 10)\n",
    "type(input)  # <class 'torch.Tensor'>\n",
    "# 檢查輸入張量的尺寸\n",
    "# 使用 PyTorch 的 size 方法，檢查 input 的形狀\n",
    "# 結果為 torch.Size([1, 10])，表示輸入有 1 個通道，特徵數為 10\n",
    "input.size()  # torch.Size([1, 10])\n",
    "# 將輸入張量傳入模型\n",
    "# 將 input 作為輸入傳遞給卷積層，得到輸出張量 output\n",
    "# 輸出大小會因卷積核大小和步幅計算而有所變化\n",
    "output = model(input)\n",
    "type(output)  # <class 'torch.Tensor'>\n",
    "output.size()  # torch.Size([1, 9])\n",
    "\n",
    "# 將卷積層改為使用三個卷積核\n",
    "model = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=2, stride=1)\n",
    "input = torch.randn(1, 10)\n",
    "type(input)  # <class 'torch.Tensor'>\n",
    "input.size()  # torch.Size([1, 10])\n",
    "output = model(input)\n",
    "type(output)  # <class 'torch.Tensor'>\n",
    "output.size()  # torch.Size([3, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循環層（Recurrent Layer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定義 LSTM 層\n",
    "model = nn.LSTM(input_size=10, hidden_size=32, num_layers=2)\n",
    "# 創建一個3維輸入張量，大小為 3 x 5 x 10，\n",
    "# 可以將這個張量想成有 3 個樣本，每個樣本是長度為 5 的時間序列，每個時間點下有 10 個特徵。\n",
    "input = torch.randn(5, 3, 10)\n",
    "type(input)  # <class 'torch.Tensor'>\n",
    "input.size()  # torch.Size([5, 3, 10])\n",
    "# 將張量input輸入模型，得到模型輸出張量output。\n",
    "output, (h_n, c_n) = model(input)\n",
    "type(output)  # <class 'torch.Tensor'>\n",
    "output.size()  # torch.Size([5, 3, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力層（Attention Layer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定義 MultiheadAttention 層\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=10, num_heads=2)\n",
    "# 創建隨機的輸入張量\n",
    "query = torch.randn(5, 3, 10)  # seq_len=5, batch_size=3, embed_dim=10\n",
    "key = torch.randn(5, 3, 10)  # seq_len=5, batch_size=3, embed_dim=10\n",
    "value = torch.randn(5, 3, 10)  # seq_len=5, batch_size=3, embed_dim=10\n",
    "# 應用 MultiheadAttention 層\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "attn_output.shape  # torch.Size([5, 3, 10])\n",
    "attn_output_weights.shape  # torch.Size([5, 3, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函式層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0163,  0.1410,  2.9745]])\n",
      "tensor([[0.2657, 0.5352, 0.9514]])\n",
      "tensor([[0.0000, 0.1410, 2.9745]])\n",
      "tensor([[-0.7684,  0.1400,  0.9948]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 創建一個隨機的輸入張量\n",
    "input_tensor = torch.randn(1, 3)\n",
    "print(input_tensor)  # tensor([-1,  1,  2])\n",
    "\n",
    "# 定義 Sigmoid 激活函式\n",
    "sigmoid = nn.Sigmoid()\n",
    "# 使用 Sigmoid 激活函式進行轉換\n",
    "output_sigmoid = sigmoid(input_tensor)\n",
    "print(output_sigmoid)  # tensor([0.2689, 0.7311, 0.8808])\n",
    "\n",
    "# 定義 ReLU 激活函式\n",
    "relu = nn.ReLU()\n",
    "# 使用 ReLU 激活函式進行轉換\n",
    "output_relu = relu(input_tensor)\n",
    "print(output_relu)  # tensor([0, 1, 2])\n",
    "\n",
    "# 定義 Tanh 激活函式\n",
    "tanh = nn.Tanh()\n",
    "# 使用 Tanh 激活函式進行轉換\n",
    "output_tanh = tanh(input_tensor)\n",
    "print(output_tanh)  # tensor([-0.7616,  0.7616,  0.9640])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 初始化 TensorBoard 紀錄器，指定紀錄內容的位置\n",
    "writer = SummaryWriter(WRITER_PATH)\n",
    "# 定義損失函式為均方誤差\n",
    "loss_fn = nn.MSELoss()\n",
    "# 定義一個 LSTM 模型\n",
    "model = nn.LSTM(input_size=10, hidden_size=32, num_layers=2)\n",
    "# 設定訓練週期的數量\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # 設置模型為訓練模式\n",
    "    train_loss = 0.0  # 初始化訓練損失值\n",
    "    # 迭代批次訓練集\n",
    "    for inputs, targets in train_loader:  # 訓練集的資料\n",
    "        # 透過 LSTM 模型進行輸入資料的預測\n",
    "        outputs, _ = model(inputs)\n",
    "        # 計算損失值\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # 清除模型參數的梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 計算參數更新的方向\n",
    "        loss.backward()\n",
    "        # 根據計算出的更新方向來更新模型參數\n",
    "        optimizer.step()\n",
    "        # 累加損失值\n",
    "        train_loss += loss.item()\n",
    "    # 計算平均訓練損失值\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    # 使用 TensorBoard 紀錄平均訓練損失值\n",
    "    writer.add_scalar(tag=\"Train Loss\", scalar_value=train_loss, global_step=epoch)\n",
    "    # 顯示每個 epoch 的平均訓練損失值\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評估函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 TensorBoard 紀錄器，指定紀錄內容的位置\n",
    "writer = SummaryWriter(WRITER_PATH)\n",
    "# 定義損失函式為均方誤差\n",
    "loss_fn = nn.MSELoss()\n",
    "# 定義一個 LSTM 模型\n",
    "model = nn.LSTM(input_size=10, hidden_size=32, num_layers=2)\n",
    "for epoch in range(EPOCHS):\n",
    "    # … 省略訓練函式的部分 …\n",
    "    # 使用 TensorBoard 紀錄平均訓練損失值\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    # 評估時禁計算梯度\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:  # 測試集的資料\n",
    "            # 透過 LSTM 模型進行輸入資料的預測\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets.unsqueeze(-1))\n",
    "            test_loss += loss.item()\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    # 使用 TensorBoard 紀錄平均測試損失值\n",
    "    writer.add_scalar(tag=\"Test Loss\", scalar_value=test_loss, global_step=epoch)\n",
    "    # 儲存模型的參數\n",
    "    torch.save(model.state_dict(), MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bookenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
